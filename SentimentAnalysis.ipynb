{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMBD Movie Reviews\n",
    "\n",
    "This project explores basic classifiers used for sentiment analysis using a dataset of 50000 movie reviews from IMDB. As my first foray into Natural Language Processing (NLP) and text analysis, much of the project serves as an exploration of text preprocessing techniques. I hope to return to this project to make improvements as I learn more from outside projects and coursework. Let's get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation, Ingestion and Preprocessing\n",
    "### Imports\n",
    "\n",
    "We'll explore the data using pandas, and use a variety of other tools to preprocess. We'll mainly use scikit-learn's classifiers to train our model. We import all necessary libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, gzip, requests, re \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import string\n",
    "import nltk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloads and Preprocessing Helpers\n",
    "\n",
    "Next, we download necessary tools from the nltk library.  \n",
    "\n",
    "1. We'll use wordnet to lemmatize each token in our text. Lemmatization removes unnecessary portions from a word so we're left with a base word that captures much of the meaning already. This could involve changing the part of speech or tense of a word, or even changing a word with a stronger connotation to a more general one. For example:\n",
    "\n",
    "<div align=\"center\">'caring' -> 'care'</div>\n",
    "<div align=\"center\">'best' -> 'good'</div>\n",
    "<div align=\"center\">'wrote' -> 'write'</div>\n",
    "\n",
    "2. We'll use stopwords to remove words that add to the dimensionality of the data and add little information. These include words like 'a', 'the', 'and', 'is', 'what', and many more. The nltk library has a preset dictionary of stopwords - later, we'll use this dictionary filter out all stopwords from each review in sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/scandukuri/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/scandukuri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/scandukuri/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/scandukuri/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/scandukuri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/scandukuri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download([ \"names\",\n",
    "                \"stopwords\",\n",
    "                \"averaged_perceptron_tagger\",\n",
    "                \"vader_lexicon\",\n",
    "                \"punkt\",\n",
    "                \"wordnet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in IMDB data from local library\n",
    "df = pd.read_csv('IMDB Dataset.csv')   \n",
    "\n",
    "# Pull in nltk lemmatizer tool and stopword dictionary\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Process each token individually after a review is split into its tokens\n",
    "def preprocess_token(token):\n",
    "    token = token.lower()   # convert to lowercase\n",
    "    if token in stopwords: \n",
    "        return ''           # leave stopwords as empty strings\n",
    "    return wnl.lemmatize(token)     # return lemmatized version of token\n",
    "\n",
    "# Process an entire movie review using helper function above\n",
    "def preprocess_doc(doc):\n",
    "    doc = re.compile(r'<.*?>').sub('', doc)     # use regex to remove html tags\n",
    "    tokens = nltk.tokenize.word_tokenize(doc)   # create list of tokens\n",
    "    tokens=[token.lower() for token in tokens if token.isalpha()]\n",
    "    words = [preprocess_token(token) for token in tokens]\n",
    "    return ' '.join((' '.join(words)).split())  # remove extra whitespace if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing, Splitting and Vectorization\n",
    "\n",
    "Now, we'll use our preprocessing functions to process our column of movie reviews, and save the cleaned text as a new column. We'll also convert our 'positive' and 'negative' labels to integers $1$ and $0$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_review'] = df['review'].apply(lambda doc : preprocess_doc(doc))\n",
    "df['sentiment_labels'] = pd.Series(df['sentiment'] == 'positive').astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll split our data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_review'], df['sentiment_labels'], random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll vectorize our reviews into word vectors so we can begin training our model. We have many options for how we want to vectorize our documents. For now, my project only explores linear classifiers, so I will ignore most options that are not applicable (i.e. word2vec).\n",
    "\n",
    "1. The Bag of Words model simply finds the union of <i>all</i> words used in <i>all</i> reviews, and for each review generates a vector of the number of times each of those words is used. \n",
    "2. The TF-IDF (Term Frequency-Inverse Document Frequency) model instead finds a metric that calculates the 'importance' of a term. It does this by multiplying the frequency of the term by the inverse of the frequency of documents that term shows up in. This results in a metric that is highest when a term is more <b>\"special\"</b> for a particular document (shows up in very few documents, and shows up a lot in a given review).\n",
    "\n",
    "We will use TF-IDF vectorization for our classifiers. The scikit-learn library has such a vectorizer that will turn our single column of text reviews into an entire table of vectors. We use this now on both our train and test data, so our testing process is easier after our model is trained later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(X_train)\n",
    "tf_x_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "First, we'll train a Multinomial Naive Bayes classifier on our data. This is a common baseline classifier used in text classification for its simplicity. It assumes that for any particular vector of token weights $\\vec{x}$ where $\\vec{x} = (x_1, x_2, \\ldots, x_n)$, and the corresponding true sentiment label $y$, the $x_i$ are conditionally independent of each other. \n",
    "\n",
    "<b>In other words, knowing the probability of the weight of a token doesn't give us any more information about the probability of the weight of any other token. </b> \n",
    "\n",
    "If the counts of each word follow a multinomial distribution, we are able to choose either the word counts themselves OR the TF-IDF vectors that we created earlier. We use the TF-IDF vectors, as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.849718221665623,\n",
       "  'recall': 0.8815981809322722,\n",
       "  'f1-score': 0.8653646871263452,\n",
       "  'support': 6157},\n",
       " '1': {'precision': 0.880726439790576,\n",
       "  'recall': 0.848652057386095,\n",
       "  'f1-score': 0.8643918105178643,\n",
       "  'support': 6343},\n",
       " 'accuracy': 0.86488,\n",
       " 'macro avg': {'precision': 0.8652223307280995,\n",
       "  'recall': 0.8651251191591836,\n",
       "  'f1-score': 0.8648782488221047,\n",
       "  'support': 12500},\n",
       " 'weighted avg': {'precision': 0.8654530318709492,\n",
       "  'recall': 0.86488,\n",
       "  'f1-score': 0.8648710106201377,\n",
       "  'support': 12500}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfMNB = MultinomialNB()\n",
    "clfMNB.fit(tf_x_train, y_train)\n",
    "MNB_y_test_pred=clfMNB.predict(tf_x_test)\n",
    "MNBReport=classification_report(y_test, MNB_y_test_pred, output_dict = True)\n",
    "\n",
    "MNBReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that our Multinomial Naive Bayes classifier has an accuracy of $86.55\\%$ on the test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC\n",
    "\n",
    "Next, we'll train a Linear Support Vector Classifier on our data. This is a type of classifier that, given $n$-vectors from each document, finds a hyperplane in $n$-dimensional space that maximizes the closest possible distance between a sample of one class and the other.\n",
    "\n",
    "In a sense, for us this means a hyperplane that gives us the maximum distance between the <b>most negative positive review</b> and the <b>most positive negative review</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.897029702970297,\n",
       "  'recall': 0.8828975150235504,\n",
       "  'f1-score': 0.8899075059343537,\n",
       "  'support': 6157},\n",
       " '1': {'precision': 0.8880434782608696,\n",
       "  'recall': 0.9016238373009617,\n",
       "  'f1-score': 0.8947821325197528,\n",
       "  'support': 6343},\n",
       " 'accuracy': 0.8924,\n",
       " 'macro avg': {'precision': 0.8925365906155833,\n",
       "  'recall': 0.892260676162256,\n",
       "  'f1-score': 0.8923448192270533,\n",
       "  'support': 12500},\n",
       " 'weighted avg': {'precision': 0.8924697331037452,\n",
       "  'recall': 0.8924,\n",
       "  'f1-score': 0.8923810864488486,\n",
       "  'support': 12500}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfLinearSVC = LinearSVC(random_state=0)\n",
    "clfLinearSVC.fit(tf_x_train,y_train)\n",
    "LinearSVC_y_test_pred=clfLinearSVC.predict(tf_x_test)\n",
    "LinearSVCReport=classification_report(y_test, LinearSVC_y_test_pred, output_dict=True)\n",
    "\n",
    "LinearSVCReport"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that our Linear SVC has an accuracy of $89.25\\%$ on the test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Conclusions\n",
    "\n",
    "For now, we've found that using TF-IDF vectorization the Linear SVC classifier performed better for sentiment analysis on IMDB movie reviews than a Naive Bayes classifier. In my next update of this project, I plan to explore another linear classifier (logistic regression) and learn more about non-linear classification (k-nearest neighbors).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14a46a681a5f6e47b50b1c99817de1726165a3a42ffd9bd5de38ad650a335b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
